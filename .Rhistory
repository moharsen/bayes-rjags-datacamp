prior_B <- rbeta(n = 10000, shape1 = 1, shape2 = 1)
# Sample 10000 draws from the Beta(100,100) prior
prior_C <- rbeta(n = 10000, shape1 = 100, shape2 = 100)
# Combine the results in a single data frame
prior_sim <- data.frame(samples = c(prior_A, prior_B, prior_C),
priors = rep(c("A","B","C"), each = 10000))
# Plot the 3 priors
ggplot(prior_sim, aes(x = samples, fill = priors)) +
geom_density(alpha = 0.5)
# Define a vector of 1000 p values
p_grid <- seq(from = 0, to = 1, length.out = 1000)
# Simulate 1 poll result for each p in p_grid
poll_result <- rbinom(1,10,p_grid)
# Create likelihood_sim data frame
likelihood_sim <- data.frame(p_grid, poll_result)
# Density plots of p_grid grouped by poll_result
ggplot(likelihood_sim, aes(x = p_grid, y = poll_result, group = poll_result)) +
geom_density_ridges()
??geom_density_ridges
install.packages("ggridges")
library(ggridges)
# Define a vector of 1000 p values
p_grid <- seq(from = 0, to = 1, length.out = 1000)
# Simulate 1 poll result for each p in p_grid
poll_result <- rbinom(1,10,p_grid)
# Create likelihood_sim data frame
likelihood_sim <- data.frame(p_grid, poll_result)
# Density plots of p_grid grouped by poll_result
ggplot(likelihood_sim, aes(x = p_grid, y = poll_result, group = poll_result)) +
geom_density_ridges()
# Define a vector of 1000 p values
p_grid <- seq(from = 0, to = 1, length.out = 1000)
# Simulate 1 poll result for each p in p_grid
poll_result <- rbinom(1000,10,p_grid)
# Create likelihood_sim data frame
likelihood_sim <- data.frame(p_grid, poll_result)
# Density plots of p_grid grouped by poll_result
ggplot(likelihood_sim, aes(x = p_grid, y = poll_result, group = poll_result)) +
geom_density_ridges()
table(poll_result)
# Density plots of p_grid grouped by poll_result
ggplot(likelihood_sim, aes(x = p_grid, y = poll_result, group = poll_result, fill = 6)) +
geom_density_ridges()
# Density plots of p_grid grouped by poll_result
ggplot(likelihood_sim, aes(x = p_grid, y = poll_result, group = poll_result, fill = poll_result==6)) +
geom_density_ridges()
sample(c("eat", "fast"),1)
?pbinom
sum(dbinom(46:54, 100, 0.5))
dbinom(45,100,0.5)
lklkhd <- function(theta){
choose(10,6)*theta^6*(1-theta)^4
}
y = lklhd(p_grid)
choose(10,6)*theta^6*(1-theta)^4
lklkhd <- function(theta){
choose(10,6)*theta^6*(1-theta)^4
}
y = lklhd(p_grid)
y = lklkhd(p_grid)
simulation <- data.frame(p_grid, y)
ggplot(simulation, aes(x = p_grid, y = y)+geom_point())
ggplot(simulation, aes(x = p_grid, y = y))+geom_point())
ggplot(simulation, aes(x = p_grid, y = y))+geom_line())
ggplot(simulation, aes(x = p_grid, y = y))+
geom_line()
lklkhd <- function(theta){
choose(10,6)*theta^6*(1-theta)^4
}
y = lklkhd(p_grid)
simulation <- data.frame(p_grid, y)
ggplot(simulation, aes(x = p_grid, y = y))+
geom_line()
lklkhd <- function(theta){
choose(10,6)*theta^6*(1-theta)^4
}
y = lklkhd(p_grid)
simulation <- data.frame(p_grid, y)
ggplot(simulation, aes(x = p_grid, y = y))+
geom_line()
### DEFINE the model
vote_model <- "model{
# Likelihood model for X
X ~ dbin(p,n)
# Prior model for p
p ~ dbeta(a,b)
}"
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)
# COMPILE the model
vote_jags <- jags.model(textConnection(vote_model),
data = list(a = 45, b = 55, X = 6, n = 10),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))
library(rjags)
### DEFINE the model
vote_model <- "model{
# Likelihood model for X
X ~ dbin(p,n)
# Prior model for p
p ~ dbeta(a,b)
}"
# COMPILE the model
vote_jags <- jags.model(textConnection(vote_model),
data = list(a = 45, b = 55, X = 6, n = 10),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)
# PLOT the posterior
plot(vote_sim, trace = FALSE)
# COMPILE the model
# COMPILE the model
vote_jags <- jags.model(textConnection(vote_model),
data = list(a = 1, b = 1, X = 6, n = 10),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))
# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)
# PLOT the posterior
plot(vote_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,18))
# COMPILE the model
vote_jags <- jags.model(textConnection(vote_model),
data = list(a = 1, b = 1, X = 220, n = 400),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))
# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)
# PLOT the posterior
plot(vote_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,18))
# COMPILE the model
vote_jags <- jags.model(textConnection(vote_model),
data = list(a = 45, b = 55, X = 220, n = 400),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))
# SIMULATE the posterior
vote_sim <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)
# PLOT the posterior
plot(vote_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,18))
install.packages("remedy")
install.packages("remedy")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(rjags)
# Take 10000 samples from the a, b, & s priors
a <- rnorm(10000,0,200)
b <- rnorm(10000,1,0.5)
s <- runif(10000,0,20)
# Store samples in a data frame
samples <- data.frame(set = 1:10000, a, b, s)
# Construct density plots of the prior samples
ggplot(samples, aes(x = a)) +
geom_density()
ggplot(samples, aes(x = b)) +
geom_density()
ggplot(samples, aes(x = s)) +
geom_density()
# Replicate the first 12 parameter sets 50 times each
prior_scenarios_rep <- bind_rows(replicate(n = 50, expr = samples[1:12, ], simplify = FALSE))
library(ggplot2)
library(rjags)
library(dplyr)
library(ggplot2)
library(rjags)
library(dplyr)
# Replicate the first 12 parameter sets 50 times each
prior_scenarios_rep <- bind_rows(replicate(n = 50, expr = samples[1:12, ], simplify = FALSE))
# Simulate 50 height & weight data points for each parameter set
prior_simulation <- prior_scenarios_rep %>%
mutate(height = rnorm(n = 600, mean = 170, sd = 102)) %>%
mutate(weight = rnorm(n = 600, mean = , sd = ___))
# Replicate the first 12 parameter sets 50 times each
prior_scenarios_rep <- bind_rows(replicate(n = 50, expr = samples[1:12, ], simplify = FALSE))
# Replicate the first 12 parameter sets 50 times each
prior_scenarios_rep <- bind_rows(replicate(n = 50, expr = samples[1:12, ], simplify = FALSE))
# Simulate 50 height & weight data points for each parameter set
prior_simulation <- prior_scenarios_rep %>%
mutate(height = rnorm(n = 600, mean = 170, sd = 102)) %>%
mutate(weight = rnorm(n = 600, mean = a+b*height, sd = s))
# Plot the simulated data & regression model for each parameter set
ggplot(prior_simulation, aes(x = height, y = weight)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, size = 0.75) +
facet_wrap(~ set)
# Replicate the first 12 parameter sets 50 times each
prior_scenarios_rep <- bind_rows(replicate(n = 50, expr = samples[1:12, ], simplify = FALSE))
# Simulate 50 height & weight data points for each parameter set
prior_simulation <- prior_scenarios_rep %>%
mutate(height = rnorm(n = 600, mean = 170, sd = 102)) %>%
mutate(weight = rnorm(n = 600, mean = a+b*height, sd = s))
# Plot the simulated data & regression model for each parameter set
ggplot(prior_simulation, aes(x = height, y = weight)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, size = 0.75) +
facet_wrap(~ set)
# Replicate the first 12 parameter sets 50 times each
prior_scenarios_rep <- bind_rows(replicate(n = 50, expr = samples[1:12, ], simplify = FALSE))
# Simulate 50 height & weight data points for each parameter set
prior_simulation <- prior_scenarios_rep %>%
mutate(height = rnorm(n = 600, mean = 170, sd = 10)) %>%
mutate(weight = rnorm(n = 600, mean = a+b*height, sd = s))
# Plot the simulated data & regression model for each parameter set
ggplot(prior_simulation, aes(x = height, y = weight)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, size = 0.75) +
facet_wrap(~ set)
View(prior_scenarios_rep)
View(prior_simulation)
install.packages("openintro")
data(openintro::bdims)
library(openintro)
data("bdims")
# Construct a scatterplot of wgt vs hgt
ggplot(bdims, aes(x = wgt, y = hgt)) +
geom_point()
# Add a model smooth
ggplot(bdims, aes(x = wgt, y = hgt)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
# Construct a scatterplot of wgt vs hgt
ggplot(bdims, aes(x = hgt, y = wgt)) +
geom_point()
# Add a model smooth
ggplot(bdims, aes(x = hgt, y = wgt)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE)
sample(c("shut down", "not necessary"),1)
install.packages("tidyverse")
install.packages("tidyverse")
weight_jags <- jags.model(
textConnection(weight_model),
data = list(Y = bdims$wgt, X = bdims$hgt),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989)
)
# DEFINE the model
weight_model <- "model{
# Likelihood model for Y[i]
for(i in 1:length(Y)) {
Y[i] ~ dnorm(m[i], s^(-2))
m[i] <- a + b*X[i]
}
# Prior models for m and s
a ~ dnorm(0, 200^(-2))
b ~ dnorm(1, 0.5^(-2))
s ~ dunif(0, 20)
}"
weight_jags <- jags.model(
textConnection(weight_model),
data = list(Y = bdims$wgt, X = bdims$hgt),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989)
)
# SIMULATE the posterior
weight_sim <- coda.samples(model = weight_jags, variable.names = c("a", "b", "s"), n.iter = 1000)
# PLOT the posterior
plot(weight_sim)
# SIMULATE the posterior
weight_sim <- coda.samples(model = weight_jags, variable.names = c("a", "b", "s"), n.iter = 100000)
# PLOT the posterior
plot(weight_sim)
# SIMULATE the posterior
weight_sim <- coda.samples(model = weight_jags, variable.names = c("a", "b", "s"), n.iter = 1000)
# PLOT the posterior
plot(weight_sim)
# SIMULATE the posterior
weight_sim_big <- coda.samples(model = weight_jags, variable.names = c("a", "b", "s"), n.iter = 100000)
# PLOT the posterior
plot(weight_sim_big)
weight_chains = data.frame(weight_sim_big[[1]])
head(weight_sim_big,20)
weight_sim_big[1]
weight_chains = data.frame(weight_sim_big[1], iter = 1:100000)
weight_chains = data.frame(as.matrix(weight_sim_big[1]), iter = 1:100000)
head(weight_chains)
# Summarize the posterior Markov chains
summary(weight_sim_big)
# Calculate the estimated posterior mean of b
mean(weight_chains$b)
# Plot the posterior mean regression model
ggplot(bdims, aes(x = hgt, y = wgt)) +
geom_point() +
geom_abline(intercept = mean(weight_chains$a), slope = mean(weight_chains$b), color = "red")
# Visualize the range of 20 posterior regression models
ggplot(bdims, aes(x = hgt, y = wgt)) +
geom_point() +
geom_abline(intercept = weight_chains$a[1:20], slope = weight_chains$b[1:20], color = "gray", size = 0.25)
# Summarize the posterior Markov chains
summary(weight_sim_big)
# Calculate the 95% posterior credible interval for b
ci_95 <- quantile(weight_sim_big$b, probs = c(0.025, 0.975))
ci_95
# Calculate the 90% posterior credible interval for b
ci_90 <- quantile(weight_sim_big$b, probs = c(0.05, 0.95))
ci_90
# Mark the 90% credible interval
ggplot(weight_chains, aes(x = b)) +
geom_density() +
geom_vline(xintercept = ci_90, color = "red")
# Summarize the posterior Markov chains
summary(weight_sim_big)
# Calculate the 95% posterior credible interval for b
ci_95 <- quantile(weight_chains$b, probs = c(0.025, 0.975))
ci_95
# Calculate the 90% posterior credible interval for b
ci_90 <- quantile(weight_chains$b, probs = c(0.05, 0.95))
ci_90
# Mark the 90% credible interval
ggplot(weight_chains, aes(x = b)) +
geom_density() +
geom_vline(xintercept = ci_90, color = "red")
# Mark 1.1 on a posterior density plot for b
ggplot(weight_chains, aes(x = b)) +
geom_density() +
geom_vline(xintercept = 1.1, color = "red")
# Summarize the number of b chain values that exceed 1.1
table(weight_chains$b>1.1)
# Calculate the proportion of b chain values that exceed 1.1
mean(weight_chains$b>1.1)
# Calculate the trend under each Markov chain parameter set
weight_chains <- weight_chains  %>%
mutate(m_180 = a + b*180)
# Construct a posterior density plot of the trend
ggplot(weight_chains, aes(x = m_180)) +
geom_density()
# Construct a posterior credible interval for the trend
quantile(weight_chains$m_180, probs = c(0.025, 0.9725))
# Calculate the trend under each Markov chain parameter set
weight_chains <- weight_chains  %>%
mutate(m_180 = a + b*180)
# Construct a posterior density plot of the trend
ggplot(weight_chains, aes(x = m_180)) +
geom_density()
# Construct a posterior credible interval for the trend
quantile(weight_chains$m_180, probs = c(0.025, 0.975))
# Simulate 1 prediction under the first parameter set
rnorm(n = 1, mean = weight_chains$m_180[1], sd = weight_chains$s[1])
# Simulate 1 prediction under the second parameter set
rnorm(n = 1, mean = weight_chains$m_180[2], sd = weight_chains$s[2])
# Simulate & store 1 prediction under each parameter set
weight_chains <- weight_chains  %>%
mutate(Y_180 = rnorm(n = 100000, mean = weight_chains$m_180, sd = weight_chains$s))
# Print the first 6 parameter sets & predictions
head(weight_chains)
# Construct a posterior credible interval for the prediction
ci_180 <- quantile(weight_chains$Y_180, probs = c(0.025, 0.975))
ci_180
# Construct a posterior credible interval for the prediction
ci_180 <- quantile(weight_chains$Y_180, probs = c(0.025, 0.975))
ci_180
# Construct a density plot of the posterior predictions
ggplot(weight_chains, aes(x = Y_180)) +
geom_density() +
geom_vline(xintercept = ci_180, color = "red")
# Visualize the credible interval on a scatterplot of the data
ggplot(weight_chains, aes(x = Y_180)) +
geom_point() +
geom_abline(intercept = mean(a), slope = mean(b), color = "red") +
geom_segment(x = 180, xend = 180, y = 59.82982, yend = 96.37724, color = "red")
# Construct a posterior credible interval for the prediction
ci_180 <- quantile(weight_chains$Y_180, probs = c(0.025, 0.975))
ci_180
# Construct a density plot of the posterior predictions
ggplot(weight_chains, aes(x = Y_180)) +
geom_density() +
geom_vline(xintercept = ci_180, color = "red")
# Visualize the credible interval on a scatterplot of the data
ggplot(bdims, aes(x = hgt, y = wgt)) +
geom_point() +
geom_abline(intercept = mean(weight_chains$a), slope = mean(weight_chains$b), color = "red") +
geom_segment(x = 180, xend = 180, y = ci_180[1], yend = ci_180[2], color = "red")
knitr::opts_chunk$set(echo = TRUE)
install.packages("mosaic")
library(rjags)
library(tidyverse)
library(mosaic)
data(RailTrail)
# Confirm that weekday is a factor variable
class(RailTrail$weekday)
# Construct a density plot of volume by weekday
ggplot(RailTrail, aes(x = volume, fill = weekday)) +
geom_density(alpha = 0.5)
rail_jags_1 <- jags.model(
textConnection(rail_model_1),
data = list(Y = RailTrail$volume, X = RailTrail$weekday),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
)
# DEFINE the model
rail_model_1 <- "model{
# Likelihood model for Y[i]
for(i in 1:len(Y){
Y[i] ~ dnorm(m[i],s^(-2))
m[i] <- a + b[X[i]]
}
# Prior models for a, b, s
a ~ dnorm(400, 100^(-2))
b[1] <- 0
b[2] ~ dnorm(0, 200^(-2))
s ~ dunif(0,200)
}"
rail_jags_1 <- jags.model(
textConnection(rail_model_1),
data = list(Y = RailTrail$volume, X = RailTrail$weekday),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
)
# DEFINE the model
rail_model_1 <- "model{
# Likelihood model for Y[i]
for(i in 1:len(Y)){
Y[i] ~ dnorm(m[i],s^(-2))
m[i] <- a + b[X[i]]
}
# Prior models for a, b, s
a ~ dnorm(400, 100^(-2))
b[1] <- 0
b[2] ~ dnorm(0, 200^(-2))
s ~ dunif(0,200)
}"
rail_jags_1 <- jags.model(
textConnection(rail_model_1),
data = list(Y = RailTrail$volume, X = RailTrail$weekday),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
)
# DEFINE the model
rail_model_1 <- "model{
# Likelihood model for Y[i]
for(i in 1:length(Y)){
Y[i] ~ dnorm(m[i],s^(-2))
m[i] <- a + b[X[i]]
}
# Prior models for a, b, s
a ~ dnorm(400, 100^(-2))
b[1] <- 0
b[2] ~ dnorm(0, 200^(-2))
s ~ dunif(0,200)
}"
rail_jags_1 <- jags.model(
textConnection(rail_model_1),
data = list(Y = RailTrail$volume, X = RailTrail$weekday),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
)
# DEFINE the model
rail_model_1 <- "model{
# Likelihood model for Y[i]
for(i in 1:length(Y)) {
Y[i] ~ dnorm(m[i], s^(-2))
m[i] <- a + b[X[i]]
}
# Prior models for a, b, s
a ~ dnorm(400, 100^(-2))
b[1] <- 0
b[2] ~ dnorm(0, 200^(-2))
s ~ dunif(0, 200)
}"
rail_jags_1 <- jags.model(
textConnection(rail_model_1),
data = list(Y = RailTrail$volume, X = RailTrail$weekday),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
)
# COMPILE the model
rail_jags_1 <- jags.model(
textConnection(rail_model_1),
data = list(Y = RailTrail$volume, X=RailTrail$weekday),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
)
# COMPILE the model
rail_jags_1 <- jags.model(
textConnection(rail_model_1),
data = list(Y = RailTrail$volume, X=RailTrail$weekday),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
)
# COMPILE the model
rail_jags_1 <- jags.model(
textConnection(rail_model_1),
data = list(Y = RailTrail$volume, X=RailTrail$weekday),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
)
# COMPILE the model
rail_jags_1 <- jags.model(
textConnection(rail_model_1),
data = list(Y = RailTrail$volume, X=RailTrail$weekday),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
)
# COMPILE the model
rail_jags_1 <- jags.model(
textConnection(rail_model_1),
data = list(Y = RailTrail$volume, X = RailTrail$weekday),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
)
# SIMULATE the posterior
rail_sim_1 <- coda.samples(model = rail_jags_1, variable.names = c("a", "b", "s"), n.iter = 10000)
# Store the chains in a data frame
rail_chains_1 <- data.frame(rail_sim_1[[1]])
# PLOT the posterior
plot(rail_sim_1)
# DEFINE the model
rail_model_1 <- "model{
# Likelihood model for Y[i]
for(i in 1:length(Y)) {
Y[i] ~ dnorm(m[i], s^(-2))
m[i] <- a + b[X[i]]
}
# Prior models for a, b, s
a ~ dnorm(400, 100^(-2))
b[1] <- 0
b[2] ~ dnorm(0, 200^(-2))
s ~ dunif(0, 200)
}"
# COMPILE the model
rail_jags_1 <- jags.model(
textConnection(rail_model_1),
data = list(Y = RailTrail$volume, X=RailTrail$weekday),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10)
)
# COMPILE the model
rail_jags_1 <- jags.model(
textConnection(rail_model_1),
data = list(Y = RailTrail$volume, X=RailTrail$weekday),
inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 1989)
)
RailTrail$weekday
